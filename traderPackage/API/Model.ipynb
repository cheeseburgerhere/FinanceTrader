{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainModel=\"../../Model/Weights/lr_dec_BiDir6200.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'../../Model/Weights/lr_dec_BiDir6200.pth': {'lookback_window': 10, 'input_size': 6, 'output_size': 1, 'num_layers': 1, 'embed_dim': 64, 'hidden_size': 128, 'bidirectional': True}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('hyperparameters.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_window = data[mainModel]['lookback_window']\n",
    "input_size = data[mainModel]['input_size']\n",
    "output_size = data[mainModel]['output_size']\n",
    "num_layers = data[mainModel]['num_layers']\n",
    "embed_dim = data[mainModel]['embed_dim']\n",
    "hidden_size = data[mainModel]['hidden_size']\n",
    "bidirectional = data[mainModel]['bidirectional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionMechanism(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionMechanism, self).__init__()\n",
    "        self.attention_weights = nn.Linear(hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, lstm_outputs):\n",
    "        # lstm_outputs: [batch_size, seq_len, hidden_dim]\n",
    "        attention_scores = self.attention_weights(lstm_outputs).squeeze(-1)  # [batch_size, seq_len]\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # [batch_size, seq_len]\n",
    "        weighted_output = torch.sum(lstm_outputs * attention_weights.unsqueeze(-1), dim=1)  # [batch_size, hidden_dim]\n",
    "        return weighted_output, attention_weights\n",
    "    \n",
    "class LSTMDoubleAttentionModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, output_dim, num_layers=1, bidirectional=False):\n",
    "        super(LSTMDoubleAttentionModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "\n",
    "        # Embedding layer (optional: use if input is categorical or needs projection)\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)  # Replace with nn.Embedding if needed\n",
    "        \n",
    "        # Encoder LSTM\n",
    "        self.encoder_lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, \n",
    "                                    batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "        # Attention\n",
    "        self.attention = AttentionMechanism(hidden_dim * self.directions)\n",
    "        \n",
    "        # Decoder LSTM\n",
    "        self.decoder_lstm = nn.LSTM(hidden_dim * self.directions, hidden_dim, num_layers,\n",
    "                                    batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(hidden_dim * self.directions, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, input_dim]\n",
    "        \n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # Encoder LSTM\n",
    "        encoder_out, _ = self.encoder_lstm(embedded)  # [batch_size, seq_len, hidden_dim * directions]\n",
    "        \n",
    "        # Attention\n",
    "        attention_out, attention_weights = self.attention(encoder_out)  # [batch_size, hidden_dim * directions]\n",
    "        \n",
    "        # Prepare Decoder Input (sequence length 1)\n",
    "        decoder_input = attention_out.unsqueeze(1)  # [batch_size, 1, hidden_dim * directions]\n",
    "        \n",
    "        # Decoder LSTM\n",
    "        decoder_out, _ = self.decoder_lstm(decoder_input)  # [batch_size, 1, hidden_dim * directions]\n",
    "        \n",
    "        # Final Prediction\n",
    "        output = self.fc(decoder_out.squeeze(1))  # [batch_size, output_dim]\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMDoubleAttentionModel(input_size, embed_dim, hidden_size, output_size, num_layers=num_layers, bidirectional=bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Determine the parent directory of traderPackage by going two levels up\n",
    "# from the current working directory (which is traderPackage/API)\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "    \n",
    "from traderPackage.Scaler import Scaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(mainModel, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "saveName=mainModel.split(\"/\")[-1].split(\".\")[0]\n",
    "saveFolder=f\"../../Model/Weights/scalers\"\n",
    "\n",
    "import pickle\n",
    "dbfile = open(os.path.join(saveFolder,f\"{saveName}/scaler_{saveName}.pkl\"), 'rb')    \n",
    "scaler = pickle.load(dbfile)\n",
    "dbfile.close()\n",
    "\n",
    "dbfile = open(os.path.join(saveFolder,f\"{saveName}/zScalerDic_{saveName}.pkl\"), 'rb')    \n",
    "zScalerDic= pickle.load(dbfile)\n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFolder=\"../../Data/StandardizedData\"\n",
    "import pandas as pd\n",
    "testData=pd.read_csv(f\"{dataFolder}/CCOLA.IS.csv\").drop(columns=[\"Pct_Change\",\"Date\"])\n",
    "zScaler=zScalerDic[\"CCOLA.IS.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Days_Between</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>31.398820</td>\n",
       "      <td>32.606818</td>\n",
       "      <td>30.971935</td>\n",
       "      <td>31.898368</td>\n",
       "      <td>6125026.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>31.898368</td>\n",
       "      <td>31.898368</td>\n",
       "      <td>30.890189</td>\n",
       "      <td>31.335241</td>\n",
       "      <td>6470728.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>31.789376</td>\n",
       "      <td>33.605913</td>\n",
       "      <td>31.444236</td>\n",
       "      <td>33.605913</td>\n",
       "      <td>6406870.8</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>33.633156</td>\n",
       "      <td>34.968312</td>\n",
       "      <td>33.333429</td>\n",
       "      <td>34.586839</td>\n",
       "      <td>6435908.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>34.586839</td>\n",
       "      <td>35.395200</td>\n",
       "      <td>33.288016</td>\n",
       "      <td>33.796647</td>\n",
       "      <td>6130689.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>33.605913</td>\n",
       "      <td>34.105460</td>\n",
       "      <td>32.697645</td>\n",
       "      <td>33.587745</td>\n",
       "      <td>5896754.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>33.587745</td>\n",
       "      <td>33.769398</td>\n",
       "      <td>31.971031</td>\n",
       "      <td>33.406091</td>\n",
       "      <td>5026230.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>33.106364</td>\n",
       "      <td>34.432436</td>\n",
       "      <td>32.670400</td>\n",
       "      <td>34.150873</td>\n",
       "      <td>7069326.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>34.196286</td>\n",
       "      <td>34.850239</td>\n",
       "      <td>33.406091</td>\n",
       "      <td>33.406091</td>\n",
       "      <td>8375281.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>33.605909</td>\n",
       "      <td>35.921995</td>\n",
       "      <td>32.970124</td>\n",
       "      <td>35.440613</td>\n",
       "      <td>10044223.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Open       High        Low      Close      Volume  Days_Between\n",
       "1000  31.398820  32.606818  30.971935  31.898368   6125026.6           1.0\n",
       "1001  31.898368  31.898368  30.890189  31.335241   6470728.0           1.0\n",
       "1002  31.789376  33.605913  31.444236  33.605913   6406870.8           3.0\n",
       "1003  33.633156  34.968312  33.333429  34.586839   6435908.6           1.0\n",
       "1004  34.586839  35.395200  33.288016  33.796647   6130689.4           1.0\n",
       "1005  33.605913  34.105460  32.697645  33.587745   5896754.6           1.0\n",
       "1006  33.587745  33.769398  31.971031  33.406091   5026230.0           1.0\n",
       "1007  33.106364  34.432436  32.670400  34.150873   7069326.0           3.0\n",
       "1008  34.196286  34.850239  33.406091  33.406091   8375281.2           1.0\n",
       "1009  33.605909  35.921995  32.970124  35.440613  10044223.2           1.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=testData.iloc[1000:1000+lookback_window,:]\n",
    "y=testData.iloc[1001+lookback_window,:]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(model,X,zScaler,scaler):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    X=X.to_numpy()\n",
    "    X=zScaler.fit_transform(X)\n",
    "    X=scaler.fit_transform(X)\n",
    "    X=torch.tensor(np.expand_dims(X,axis=0), dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        X = X.to(device)\n",
    "        outputs, att = model(X)\n",
    "\n",
    "    zz=np.zeros((1,6))\n",
    "    zz[0,3]=outputs[0,0].to(\"cpu\").numpy()\n",
    "    \n",
    "    normalized=zScaler.inverse_transform(scaler.inverse_transform(zz))\n",
    "    return normalized[0,3], att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 34.76030126226647  Real Value: 38.04734090909091\n"
     ]
    }
   ],
   "source": [
    "a,att=answer(model,X,zScaler,scaler)\n",
    "print(\"Prediction:\",a,\" Real Value:\",y.iloc[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
